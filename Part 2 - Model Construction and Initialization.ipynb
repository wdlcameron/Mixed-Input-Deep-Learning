{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import imageio\n",
    "from skimage import io, transform\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from scripts.dataloader import Dataset, Transforms, Resize, ToTorch, Sampler, collate, DataLoader\n",
    "from functools import partial\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 - Imports from the Part 1\n",
    "\n",
    "Now that we have a dataloader, we can start to build model that uses that data.  Before we begin, let us create a dataset and dataloader from which we'll extract the minibatches required to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = r'data/processed_dataframe.csv'\n",
    "img_col = 'filename'\n",
    "cont_cols = ['followers', 'following', 'engagement_factor_std', 'month', 'year', 'day_name', 'hour']\n",
    "cat_cols = []\n",
    "target_col = 'engagement_factor_moving_avg'\n",
    "image_path = Path(r'data/Images')\n",
    "tfms = Transforms([Resize(256), ToTorch()])\n",
    "\n",
    "ds_train = Dataset(df_path, \n",
    "                   img_col = img_col,\n",
    "                   cont_cols = cont_cols, \n",
    "                   cat_cols = cat_cols, \n",
    "                   target_col = target_col, \n",
    "                   image_path = image_path, \n",
    "                   transforms = tfms)\n",
    "\n",
    "dl_train = DataLoader(dataset = ds_train,\n",
    "                      sampler = Sampler(ds_train, bs = 16),\n",
    "                      collate_func = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch 0, with target shape torch.Size([16])\n",
      "Minibatch 1, with target shape torch.Size([16])\n",
      "Minibatch 2, with target shape torch.Size([16])\n",
      "Minibatch 3, with target shape torch.Size([16])\n",
      "Minibatch 4, with target shape torch.Size([16])\n",
      "Minibatch 5, with target shape torch.Size([16])\n",
      "Minibatch 6, with target shape torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for i, (xb,yb) in enumerate(dl_train):\n",
    "    print (f\"Minibatch {i}, with target shape {yb.shape}\")\n",
    "    if i>5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dl_train)\n",
    "xb, yb = next(iterator)\n",
    "x_image, x_tab = xb\n",
    "x_image = x_image.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 256, 256]), torch.Size([16, 7]), torch.Size([16]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_image.shape, x_tab.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a single batch to work with as we test our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Investigating the components of an nn.Module\n",
    "\n",
    "Pytorch models are created using nn.Module as a base class.  This imparts many features onto the model and its subcomponents, but at the core it allows a few things to happen:\n",
    "- You can initialize the module with __init__ to store all the key variables as well as a call to the nn.Module.__init__ using super. That will set up many of the core instance variables of the module (including the parameters that will be tracked and updated during training)\n",
    "- You can define how an input is processed by the model and what it outputs (should output a torch tensor).  This is the \"forward\" class method and we will use it extensively when constructing our models\n",
    "- You can register forwards or backwards hooks (extra code that will be run during the forward or backwards phase).  These (and callbacks in general) will be the subject of a future instalment\n",
    "- Other useful class methods such as zero_grad, or the ability to set the state as training or validation (eval)\n",
    "\n",
    "Note: rather than have a backwards class, pytorch uses an autograd system to update the gradients for all the relevant parameters, and an optimizer class to update the parameters themselves.  We will go over these features in a future installment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, one of the basic modules is the nn.Linear class, which performs the operation wx + b, where w is the weight parameter and b is the bias.  Not that for historical reasons the operation is performed as x.(w.T) + b, so the shape of the weights will be the opposite of what you expect\n",
    "\n",
    "```python\n",
    "class Linear(Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:y = xA^T + b\"\"\"\n",
    "    __constants__ = ['bias', 'in_features', 'out_features']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "```\n",
    "\n",
    "The F.linear is defined as:\n",
    "\n",
    "\n",
    "```python \n",
    "def linear(input, weight, bias=None):\n",
    "    if input.dim() == 2 and bias is not None:\n",
    "        # fused op is marginally faster\n",
    "        ret = torch.addmm(bias, input, weight.t())\n",
    "    else:\n",
    "        output = input.matmul(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias\n",
    "        ret = output\n",
    "    return ret\n",
    "\n",
    "```\n",
    "\n",
    "Which is essentially x.(w.T) + b for our purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that initializing the module creates the appropriate weight and bias tensors, then initializes them using kaiming_uniform.  If you want to initialize your parameters in a different way, you can either modify them after the layer has been created (which we will discuss later), or create another Linear class that overwrites the reset_parameters class method.  \n",
    "\n",
    "We can explore the input and output of the linear module using a test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 20]), torch.Size([16, 5]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = torch.rand(16, 20)\n",
    "test_linear_layer = nn.Linear(20, 5)\n",
    "results = test_linear_layer(test_batch)\n",
    "test_batch.shape, results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we input a tensor with batch size of 16 and with 20 elements, and that after going through the linear layer we are left with 5 elements from our of our 16 input samples, as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - nn.module Building Blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful modules \n",
    "\n",
    "The torch.nn.Module class provides  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- nn.Linear() - The basic building block of a fully-connected network.  It will take an input with shape (batch_size, in_features) and will produce an output with shape (batch_size, out_features)\n",
    "    * in_features - the number of features coming into the module (independent of batch size) \n",
    "    * out_features - the number of desired features leaving the module\n",
    "    * Note: nn.Linear will have up to two parameters:\n",
    "        - A weight matrix of shape (out_features, in_features).\n",
    "        - A bias vector of shape (out_features)\n",
    "\n",
    "\n",
    "- nn.Conv2d() - The basic building block of a convolutional neural net.  It takes in an input with shape (batch_size, in_channels, height, width), and will produce an output with shape (batch_size, out_channels, height, width)\n",
    "    * in_channels - The number of channels coming into the layer.  \n",
    "    * out_channels - The number of channels leaving the layer\n",
    "    * kernel_size,\n",
    "    * stride=1,\n",
    "    * padding=0,\n",
    "    * dilation=1,\n",
    "    * groups=1,\n",
    "    * bias=True,\n",
    "    * padding_mode='zeros',\n",
    "    * Note: nn.Conv2d has up to two types of parameters:\n",
    "        - The kernel with shape (out_channels, in_channels, kernel_size, kernel_size).  Another way to think about this is that there are \"out_channel\" kernels of shape (in_channels, kernel_size, kernel_size), which each produce a single channel of the output\n",
    "        - A bias vector of shape (out_channels).  Each channel will receive a single bias, which will be applied to each pixel of the channel.\n",
    "    \n",
    "    \n",
    "- nn.ReLU()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear node\n",
      "torch.Size([10, 5])\n",
      "torch.Size([10])\n",
      "\n",
      "Conv2d\n",
      "torch.Size([20, 10, 3, 3])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "def param_shapes(module):\n",
    "    for p in module.parameters():\n",
    "        print (p.shape)\n",
    "\n",
    "print ('Linear node')\n",
    "param_shapes(nn.Linear(5, 10))\n",
    "print('')\n",
    "\n",
    "print ('Conv2d')\n",
    "param_shapes(nn.Conv2d(10,20,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create our own modules by inheriting from the nn.Modules class.  The key requirements when inheriting from the nn.Modules class is that you call the superclass's initialization, and that you define a forward() class method.  Generally, this forward class method accepts a tensor as an input and returns a tensor as an output.  Thanks to the autograd feature of pytorch, you can do whatever you want within those two requirements as tensors will track their own grads throughout the process.  \n",
    "\n",
    "A useful class to make is a Lambda module, which accepts a function during class initialization and applies that function to the input tensor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "    \n",
    "def flatten(x):\n",
    "    return x.view(x.shape[0], -1)\n",
    "\n",
    "def simulate_fc_output(x, n):\n",
    "    return torch.rand((x.shape[0], n))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Creating a Custom Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building with the end in mind\n",
    "\n",
    "We don't have all the components that we need for the final model, but we can construct our mixed model using some simulated components.  Those components are:\n",
    "- The tabular model\n",
    "- The CNN/image model\n",
    "- The mixed model (that combines the two outputs)\n",
    "\n",
    "Once we have all of these models, we can combine them into a MixedInputModel class, where we call each model sequentially and by passing in the appropriate inputs for each submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedInputModel(nn.Module):\n",
    "    def __init__(self, cnn_model,  tabular_model, mixed_model):\n",
    "        super(MixedInputModel, self).__init__()\n",
    "        \n",
    "        self.cnn_model = cnn_model\n",
    "        self.tabular_model = tabular_model\n",
    "        self.mixed_model = mixed_model\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #unpack the x_batch tuple into the image and tabular components\n",
    "        x_image, x_tab = x\n",
    "        x_image = x_image.float()\n",
    "        x_tab = x_tab.float()\n",
    "        \n",
    "        #run each component seperately through their respective models\n",
    "        cnn_output = self.cnn_model(x_image)\n",
    "        tabular_output = self.tabular_model(x_tab)\n",
    "        \n",
    "        #concatenate the outputs from both networks and pass it through the mixed model output\n",
    "        concat_outputs = torch.cat((cnn_output, tabular_output), dim = 1)\n",
    "        mixed_model_output = self.mixed_model(concat_outputs) \n",
    "        \n",
    "        return(mixed_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Lambda(partial(simulate_fc_output, n = 10))\n",
    "tabular_model = Lambda(partial(simulate_fc_output, n = 5))\n",
    "mixed_model = Lambda(partial(simulate_fc_output, n = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10]),\n",
       " torch.Size([16, 5]),\n",
       " torch.Size([16, 15]),\n",
       " torch.Size([16, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_output = cnn_model(x_image)\n",
    "tabular_output = tabular_model(x_tab)\n",
    "concat_outputs = torch.cat((cnn_output, tabular_output), dim = 1)\n",
    "mixed_model_output = mixed_model(concat_outputs)\n",
    "\n",
    "(cnn_output.shape, tabular_output.shape, concat_outputs.shape, mixed_model_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = MixedInputModel(cnn_model = cnn_model,\n",
    "                              tabular_model = tabular_model,\n",
    "                              mixed_model = mixed_model\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to mixing the two inputs!  Unfortunately, we won't get great results from this model since the submodels just create constant outputs.  We now need to create the three input models to replace our placeholder models: the cnn_model, the tabular_model, and the mixed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Tabular Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTabularModel(nn.Module):\n",
    "    h1 = 10\n",
    "    h2 = 5    \n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(BasicTabularModel, self).__init__()\n",
    "        self.layer_1 = nn.Linear(in_features, self.h1)\n",
    "        self.layer_2 = nn.Linear(self.h1, self.h2)\n",
    "        self.layer_3 = nn.Linear(self.h2, out_features)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, in_features = x_tab.shape\n",
    "basic_model = BasicTabularModel(in_features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2127.0574],\n",
       "        [ -2126.9705],\n",
       "        [ -1160.0604],\n",
       "        [ -1527.4480],\n",
       "        [ -6532.2085],\n",
       "        [ -9353.8359],\n",
       "        [ -1160.0170],\n",
       "        [ -2126.8796],\n",
       "        [ -8053.3960],\n",
       "        [ -1527.5387],\n",
       "        [ -1527.5488],\n",
       "        [-15028.7510],\n",
       "        [ -5070.6572],\n",
       "        [ -1326.1842],\n",
       "        [ -7952.2861],\n",
       "        [ -8053.3062]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_model(x_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can define a complete model during the class initialization and then use that during the forward pass.  This approach will make things easier when we put everything together later on in the notebook.  We can also make the model more flexible by accepting the tabular models hidden and final layer sizes as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):    \n",
    "    def __init__(self, layer_sizes):\n",
    "        super(TabularModel, self).__init__()\n",
    "        \n",
    "        layers = []      \n",
    "        \n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        \n",
    "        self.model = nn.Sequential(*layers[:-1]) #ignore the last nn.ReLU\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, in_features = x_tab.shape\n",
    "tab_model = TabularModel([in_features, 10, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[112.6839],\n",
       "        [112.8799],\n",
       "        [ 72.8295],\n",
       "        [ 90.7563],\n",
       "        [322.1517],\n",
       "        [388.5081],\n",
       "        [ 73.1328],\n",
       "        [112.7487],\n",
       "        [353.0711],\n",
       "        [ 90.7738],\n",
       "        [ 90.7262],\n",
       "        [618.0038],\n",
       "        [289.4448],\n",
       "        [ 88.8436],\n",
       "        [339.6225],\n",
       "        [353.2600]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_model(x_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with a known number of inputs, we have created a tabular model that produces a single output value.  Notably, these values are much larger than the -1 to 1 range that we usually expect.  This is because the weights of the network have not been properly initialized.  We will handle network initialization later.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 3, 256, 256)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs, c, w, h = x_image.shape\n",
    "bs, c, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNModel(nn.Module):\n",
    "    c1 = 4\n",
    "    c2 = 8\n",
    "    c3 = 12\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNNModel, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Conv2d(3, self.c1, kernel_size = 3, padding = 1, stride = 2), nn.ReLU(),\n",
    "                                  nn.Conv2d(self.c1, self.c2, kernel_size = 3, padding = 1, stride = 2), nn.ReLU(), \n",
    "                                  nn.Conv2d(self.c2, self.c3, kernel_size = 3, padding = 1, stride = 2), nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 12, 32, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_cnn_model = SimpleCNNModel()\n",
    "simple_cnn_model(x_image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that we have the desired 12 output channels. The height and width of our output image has changed.  After each convolutional layer, the output height and width will change in the following manner: \n",
    "\n",
    "$$ output\\_size = \\frac{input\\_size + 2*padding - dilation*(kernel\\_size-1) -1}{stride} + 1 $$\n",
    "\n",
    "Although this is not something that we need to concern ourselves with during the input stage, it's necessary to understand how these values change so that when we want to unflatten the image into a fully connected layer, we know how many input layers there are.  Tracking the size also lets us customize the CNN component such that it brings the final image down to an appropriate size.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in the Fully Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "    \n",
    "def flatten(x):\n",
    "    return x.view(x.shape[0], -1)\n",
    "    \n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    h = [5, 7, 10, 14] #hidden layer channels    \n",
    "    def __init__(self, img_channels, img_size):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.img_channels, self.size = img_channels, img_size\n",
    "        \n",
    "        current_channels = img_channels\n",
    "        output_size = img_size\n",
    "        \n",
    "        cnn_model_components = []\n",
    "        for new_channels in self.h:\n",
    "            layer, output_size = self.get_cnn_layer(current_channels, new_channels, 3, 1, 2, output_size)\n",
    "            current_channels = new_channels\n",
    "            cnn_model_components.append(layer)\n",
    "        \n",
    "        while output_size >5:\n",
    "            layer, output_size = self.get_cnn_layer(self.h[-1], self.h[-1], 3, 1, 2, input_size = output_size)\n",
    "            \n",
    "        cnn_model_components.append(nn.AdaptiveAvgPool2d(1))\n",
    "        cnn_model_components.append(Lambda(flatten))\n",
    "        \n",
    "        fcc_model_components = nn.Sequential(nn.Linear(self.h[-1], 20), nn.ReLU(),\n",
    "                                            nn.Linear(20, 10), nn.ReLU(),\n",
    "                                            nn.Linear(10, 1))      \n",
    "        \n",
    "        self.model = nn.Sequential(*cnn_model_components, fcc_model_components)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    def get_cnn_layer(self, inp_chs, out_chs, kernel_size = 3, padding = 1, stride = 2, input_size = None):\n",
    "        \"\"\"\n",
    "        This function acts as a default for \n",
    "        \n",
    "        \n",
    "        We can keep track of the final size of our network based on the initial size.  The formula for output\n",
    "        size is the floor of O = ((Input_size + 2*padding - dilation*(kernel_size-1) -1)/stride) + 1.  For instance, with\n",
    "        input size = 256, kernel_size = 3, padding = 2 and stride = 2, we get (256+2*2-1*(3-1)-1)/2 = 127.5, whose floor\n",
    "        is 127.  We therefore expect the output height and width to be 127\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        cnn_layer = nn.Conv2d(inp_chs, out_chs, kernel_size, stride, padding,1)\n",
    "        \n",
    "        if input_size is None: return cnn_layer\n",
    "        else:\n",
    "            new_size = ((input_size + 2*padding - 1*(kernel_size - 1) -1)/stride + 1)//1\n",
    "            return cnn_layer, new_size\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 5, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): Conv2d(5, 7, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (2): Conv2d(7, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): Conv2d(10, 14, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): AdaptiveAvgPool2d(output_size=1)\n",
       "    (5): Lambda()\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=14, out_features=20, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=20, out_features=10, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=10, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = CNNModel(3, h)\n",
    "cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1628],\n",
      "        [-0.1622],\n",
      "        [-0.1626],\n",
      "        [-0.1630],\n",
      "        [-0.1630],\n",
      "        [-0.1624],\n",
      "        [-0.1625],\n",
      "        [-0.1622],\n",
      "        [-0.1627],\n",
      "        [-0.1625],\n",
      "        [-0.1632],\n",
      "        [-0.1631],\n",
      "        [-0.1632],\n",
      "        [-0.1635],\n",
      "        [-0.1630],\n",
      "        [-0.1627]], grad_fn=<AddmmBackward>) \n",
      "\n",
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "predictions = cnn_model(x_image)\n",
    "print (predictions,'\\n')\n",
    "print (predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting a Pretrained Model\n",
    "\n",
    "We are now able to output a single prediction from each image of our image batch.  However, although you can get great results from a simple tabular model, it's unlikely that we will be able to get good predictive results from a network this simple for image processing.  Fortunately, there are many existing cnn_models that we can use.  An added bonus is the ability to use pretrained networks, which allows us to benefit from weights that have been trained on huge datasets of images.  Although we may not ultimately use our model for the same application, many of the earlier kernels will provide detection of more universal components.  \n",
    "\n",
    "Now that we know how to build a model from components, we can start to mix in these preconstructed and pretrained CNN models into our final model.  torchvision.models provides many models to choose from.  We will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = torchvision.models.resnet34(pretrained=True)\n",
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through the model, we see that there are four types of nn.Modules used.  Some are familiar to us and others are new.\n",
    "\n",
    "Known:\n",
    "- Conv2d\n",
    "- Linear\n",
    "- ReLU\n",
    "\n",
    "New:\n",
    "- BatchNorm2d\n",
    "- BasicBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from our base model that the final output is a fully-connected layer with 1000 outputs.  That is a very useful output for our purposes, since out final prediction requires only a single output.  That means we have a lot of flexibility between the 1000 inputs and the single output to implement our model.  Before we begin, let us test that this model handles our image batch correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1000])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = base_model(x_image)\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our output is of the form (batch_size, 1000).  That means we're good to go in terms of incorporating Resnet34 into our final cnn model.  As before, we'll add in some Linear layers in order to gradually bring the final number of nodes down to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResnet(nn.Module):\n",
    "    def __init__(self, base_model, connected_layer_sizes):\n",
    "        super(CustomResnet, self).__init__()\n",
    "        #self.base_mode = base_model\n",
    "        #self.model_outputs = model_outputs\n",
    "        self.connected_layer_sizes = connected_layer_sizes\n",
    "        \n",
    "        connected_layers = []\n",
    "        for i in range(len(connected_layer_sizes)-1):\n",
    "            h1 = connected_layer_sizes[i]\n",
    "            h2 = connected_layer_sizes[i+1]\n",
    "            \n",
    "            connected_layers.append(nn.Linear(h1, h2))\n",
    "            connected_layers.append(nn.ReLU())\n",
    "            \n",
    "        connected_model = nn.Sequential(*connected_layers)\n",
    "        \n",
    "        \n",
    "        self.model = nn.Sequential(base_model, connected_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1394],\n",
       "        [0.0000],\n",
       "        [0.1743],\n",
       "        [0.1177],\n",
       "        [0.0000],\n",
       "        [0.1775],\n",
       "        [0.0498],\n",
       "        [0.0000],\n",
       "        [0.0991],\n",
       "        [0.1745],\n",
       "        [0.1705],\n",
       "        [0.0022],\n",
       "        [0.0502],\n",
       "        [0.0000],\n",
       "        [0.1895],\n",
       "        [0.0634]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_resnet = CustomResnet(torchvision.models.resnet34(pretrained = True), [1000, 50, 20, 1])\n",
    "cnn_model_resnet(x_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side Note - Further Modifications\n",
    "\n",
    "We were fortunate that the output of the base model was conveniently sized, but what if this was not the case?  For instance, if the model had a fully connected component that tailored the output to a very specific application.  Fortunately, we can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child: 0\n",
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) \n",
      "\n",
      "Child: 1\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n",
      "\n",
      "Child: 2\n",
      "ReLU(inplace=True) \n",
      "\n",
      "Child: 3\n",
      "MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) \n",
      "\n",
      "Child: 4\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "Child: 5\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "Child: 6\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (4): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (5): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "Child: 7\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "Child: 8\n",
      "AdaptiveAvgPool2d(output_size=(1, 1)) \n",
      "\n",
      "Child: 9\n",
      "Linear(in_features=512, out_features=1000, bias=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(base_model.children()):\n",
    "    print('Child:', i)\n",
    "    print (c, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert these children into a list and select only the children that we want.  Note that each nn.Sequential (or module) will be grouped as a single child.  Each of these children may have children of its own.  For instance, Child 4 has three children, each a BasicBlock.  Each of these basic blocks has 5 children of their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 8, 8])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "children = list(base_model.children())\n",
    "new_model = nn.Sequential(*children[:-2])\n",
    "results = new_model(x_image)\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By extracting children of this pretrained model, we can use only the components that we need.  Since this is a pretrained model, however, it makes more sense to cut out layers from the end rather than the beginning.  The middle layers have been trained to accept the inputs from their previous layers.  If those are gone you may lose the benefit of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Putting it all Together\n",
    "\n",
    "Now that we have all the components in place, we will construct our model from the following components:\n",
    "- A tabular model (fully connected) that accepts the tabular inputs and outputs 4 numbers\n",
    "- A CNN model that accepts the image inputs and outputs 10 nodes\n",
    "- Another fully connected model that accepts the concatenated outputs from the tabular and cnn (14 total), and results in a single predictive output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, ch_img, h_img, w_img = x_image.shape\n",
    "bs, tab_inputs = x_tab.shape\n",
    "\n",
    "num_cnn_outputs = 10\n",
    "num_tabular_outputs = 4\n",
    "\n",
    "num_mixed_inputs = num_cnn_outputs + num_tabular_outputs\n",
    "\n",
    "\n",
    "input_cnn_model = CustomResnet(torchvision.models.resnet34(pretrained = True), [1000,50,20, num_cnn_outputs])\n",
    "input_tabular_model = TabularModel([tab_inputs, 10, num_tabular_outputs])\n",
    "input_mixed_model = TabularModel([num_mixed_inputs, 7, 1])\n",
    "\n",
    "mixed_model = MixedInputModel(input_cnn_model, input_tabular_model, input_mixed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 177.6637],\n",
       "        [ 177.6346],\n",
       "        [  97.0397],\n",
       "        [ 127.4798],\n",
       "        [ 544.0270],\n",
       "        [ 781.1898],\n",
       "        [  97.0436],\n",
       "        [ 177.5530],\n",
       "        [ 672.1700],\n",
       "        [ 127.5017],\n",
       "        [ 127.5086],\n",
       "        [1254.7173],\n",
       "        [ 421.1671],\n",
       "        [ 110.4783],\n",
       "        [ 663.9909],\n",
       "        [ 672.1181]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we enumerate through the children, we see that each of our models is a separate child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child: 0\n",
      "CustomResnet(\n",
      "  (model): Sequential(\n",
      "    (0): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (5): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1000, out_features=50, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=50, out_features=20, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=20, out_features=10, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      ") \n",
      "\n",
      "Child: 1\n",
      "TabularModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=4, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "Child: 2\n",
      "TabularModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=14, out_features=7, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=7, out_features=1, bias=True)\n",
      "  )\n",
      ") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(mixed_model.children()):\n",
    "    print('Child:', i)\n",
    "    print (c, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - A Note on Initializing our Network\n",
    "\n",
    "The first few cycles of training are crucial as inproper starting weights can cause the gradients to vanish (go to zero) or explode (go towards infinity).  This occurs when the outputs from each layer tend away from a mean of zero and a std of 1.  There are a few methods that models use to address this.  In many CNN models, there is a BatchNorm layer, which essentially resets the batch to have a mean of 0 and std of 1, then scales it using some learned parametes.\n",
    "\n",
    "Another helpful way to reduce these issues is to ensure that you initialize your parameters correctly based on an understanding of how the mean and standard deviation change after each layer.  We saw earlier that the linear component of our network is initialized using kaiming_uniform and that the CNN component is built using transfer leaarning.  This suggests that we should be alright in terms of getting things trained.\n",
    "\n",
    "If you do want to do your own initialization, you can either create your own versions of the layers (as we discussed earlier with the nn.Linear class and the reset_parameters() class method).  Alternatively, you can re-initialize any of the parameters once your model has been created.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take our cnn_model, for example, we can look at all of the children it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 5, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (1): Conv2d(5, 7, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (2): Conv2d(7, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (3): Conv2d(10, 14, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (4): AdaptiveAvgPool2d(output_size=1)\n",
       "  (5): Lambda()\n",
       "  (6): Sequential(\n",
       "    (0): Linear(in_features=14, out_features=20, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=10, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last child (6) is the one we're interested in.  We can extract that by converting the children into a list and taking only the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcc_components = list(cnn_model.model.children())[-1]\n",
    "all_children = list(fcc_components.children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to reinitialize all of the weights from this sections, we can access them through the .weight attribute for each child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=14, out_features=20, bias=True): Has a weight parameter\n",
      "ReLU(): Does not have a weight parameter\n",
      "Linear(in_features=20, out_features=10, bias=True): Has a weight parameter\n",
      "ReLU(): Does not have a weight parameter\n",
      "Linear(in_features=10, out_features=1, bias=True): Has a weight parameter\n"
     ]
    }
   ],
   "source": [
    "for c in all_children:\n",
    "    print(c, end = ': ')\n",
    "    if hasattr(c, 'weight'): print(\"Has a weight parameter\")\n",
    "    else: print(\"Does not have a weight parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = []\n",
    "for c in all_children:\n",
    "    if hasattr(c, 'weight'): all_weights.append(c.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1378, -0.2137, -0.2102, -0.1990, -0.2235, -0.0833,  0.1377, -0.0089,\n",
       "           0.0644, -0.0759,  0.0710, -0.0770, -0.1365, -0.2226],\n",
       "         [-0.2377,  0.2290,  0.1606, -0.0603,  0.1496, -0.0753,  0.0509,  0.2001,\n",
       "           0.1397,  0.0572,  0.2463,  0.0973,  0.0753, -0.1579],\n",
       "         [ 0.0715, -0.1806, -0.2265, -0.0886,  0.0036, -0.0915, -0.1257,  0.2489,\n",
       "          -0.0050, -0.2272, -0.1134, -0.0693,  0.2226,  0.0880],\n",
       "         [-0.2626,  0.2100,  0.0514,  0.1719,  0.1605,  0.2494,  0.2279, -0.2015,\n",
       "          -0.2011, -0.2477, -0.0011,  0.1262,  0.2447,  0.2086],\n",
       "         [-0.2361,  0.0594, -0.1388,  0.0705,  0.2034, -0.0841,  0.2036, -0.2455,\n",
       "          -0.0587,  0.0263,  0.0083, -0.2358, -0.0099, -0.0145],\n",
       "         [ 0.1850,  0.0300,  0.1793,  0.0206,  0.2030,  0.0741, -0.2363,  0.0554,\n",
       "           0.0657,  0.1588, -0.0977, -0.0056, -0.2349, -0.1058],\n",
       "         [-0.1851,  0.1246,  0.1244, -0.2287,  0.1660,  0.2034, -0.0282, -0.0226,\n",
       "           0.1138,  0.1852,  0.0199, -0.0156, -0.1116,  0.0228],\n",
       "         [-0.1727, -0.1962, -0.0115, -0.1588,  0.0489, -0.1443, -0.0788, -0.1635,\n",
       "           0.0326, -0.0172, -0.2607, -0.1406,  0.0032, -0.1202],\n",
       "         [-0.2297, -0.0121, -0.2631,  0.0821,  0.2116,  0.0117, -0.0565, -0.0343,\n",
       "           0.1159,  0.2406,  0.1871,  0.0023,  0.2649,  0.0052],\n",
       "         [-0.2092,  0.1998, -0.0665,  0.1018,  0.2572,  0.1210, -0.1415, -0.1219,\n",
       "           0.1876,  0.2154,  0.2303, -0.2371,  0.2206,  0.1871],\n",
       "         [ 0.2357, -0.2447,  0.0336, -0.1496, -0.0315,  0.0046,  0.1805, -0.1399,\n",
       "           0.1750,  0.2467,  0.0599, -0.1733, -0.0789, -0.1515],\n",
       "         [-0.2406,  0.1389, -0.0523, -0.0230, -0.0082, -0.2613, -0.0132,  0.1489,\n",
       "           0.1666, -0.1177,  0.1452,  0.0340, -0.1651,  0.0653],\n",
       "         [ 0.1702, -0.2636,  0.1759, -0.2125, -0.0167, -0.2308,  0.0079,  0.2230,\n",
       "          -0.1735, -0.0428,  0.2339, -0.1077, -0.2070, -0.2620],\n",
       "         [-0.0440, -0.1693, -0.1734,  0.0233,  0.1212, -0.2590,  0.0934, -0.0668,\n",
       "           0.0180,  0.0598, -0.0547, -0.2494, -0.1278,  0.1997],\n",
       "         [ 0.1830, -0.2595, -0.0195,  0.1997,  0.1651, -0.0914, -0.1088, -0.0335,\n",
       "          -0.1729,  0.0456,  0.2222, -0.1224,  0.2139, -0.0348],\n",
       "         [-0.0510, -0.1084, -0.1064, -0.2315, -0.2295, -0.0835, -0.1747, -0.1962,\n",
       "          -0.2592, -0.1599,  0.1270,  0.0603, -0.0577,  0.2020],\n",
       "         [ 0.0015,  0.2504, -0.1160,  0.0212, -0.1214,  0.1944, -0.1219, -0.2237,\n",
       "          -0.0481, -0.0616, -0.1871, -0.2237, -0.1712,  0.0603],\n",
       "         [-0.1378, -0.1705, -0.0226, -0.2065,  0.0511,  0.1290,  0.1711,  0.1682,\n",
       "          -0.1900, -0.0028, -0.0546,  0.1012,  0.0529, -0.1022],\n",
       "         [ 0.2638, -0.0005, -0.1939,  0.2425, -0.2017,  0.0984, -0.2111,  0.2095,\n",
       "           0.1863, -0.0380, -0.0702,  0.0577,  0.0003, -0.0749],\n",
       "         [ 0.2208, -0.1777, -0.2655,  0.2409,  0.0328,  0.1598,  0.1222,  0.1816,\n",
       "           0.2370,  0.1242,  0.0317,  0.2069,  0.1947,  0.1066]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([[-0.1144, -0.1101,  0.0272, -0.0263, -0.0348,  0.1200,  0.0378,  0.1006,\n",
       "          -0.2066, -0.2199, -0.0820, -0.0667, -0.1858, -0.0601,  0.0875, -0.2202,\n",
       "          -0.0795, -0.0076,  0.2032, -0.1065],\n",
       "         [ 0.0204, -0.1330, -0.1860,  0.1558, -0.0091,  0.1463, -0.1495,  0.0211,\n",
       "           0.1092,  0.1286, -0.0732, -0.2100,  0.0023,  0.1149, -0.1034, -0.0216,\n",
       "           0.0433,  0.1256,  0.1120, -0.1774],\n",
       "         [-0.0274,  0.1158, -0.1616, -0.1633, -0.1821,  0.0607,  0.0971,  0.1802,\n",
       "           0.0977,  0.0707, -0.1847, -0.1271,  0.1370, -0.0658,  0.1214,  0.1295,\n",
       "           0.1682, -0.2097, -0.0662, -0.0281],\n",
       "         [ 0.1169, -0.0439,  0.1790, -0.1800, -0.0351,  0.0936,  0.0646, -0.1468,\n",
       "           0.0887, -0.0334, -0.0179, -0.0987,  0.0452,  0.1952, -0.1409,  0.0057,\n",
       "           0.1084, -0.0850,  0.1249,  0.0678],\n",
       "         [-0.0497, -0.1332, -0.1631,  0.1217,  0.0884,  0.1615, -0.1192, -0.0716,\n",
       "          -0.1065, -0.1034, -0.1314,  0.0059, -0.0352,  0.0838, -0.1974,  0.0678,\n",
       "           0.1673,  0.2007,  0.0858, -0.1783],\n",
       "         [-0.0746, -0.2158,  0.0753, -0.1334,  0.0463, -0.0636, -0.1287,  0.0862,\n",
       "           0.2231, -0.1674, -0.0751,  0.0008, -0.0061, -0.0454,  0.0992,  0.1273,\n",
       "           0.0431, -0.1153,  0.2030, -0.0665],\n",
       "         [ 0.0830, -0.0414, -0.1589,  0.0125,  0.1072,  0.0809, -0.0897, -0.1423,\n",
       "          -0.0213,  0.2135,  0.0969, -0.0391,  0.1432,  0.2063, -0.0006, -0.0071,\n",
       "          -0.2070, -0.1674, -0.0164,  0.0517],\n",
       "         [-0.0650,  0.1809, -0.0709, -0.1402, -0.0582,  0.0287, -0.0631, -0.1789,\n",
       "           0.1560, -0.1393, -0.0697, -0.1266, -0.1460, -0.0857, -0.1460,  0.0302,\n",
       "          -0.0051,  0.1647, -0.1093,  0.1961],\n",
       "         [ 0.2085,  0.1437, -0.0091,  0.1118,  0.0087, -0.1203,  0.1868,  0.1688,\n",
       "           0.2071, -0.0247,  0.0681, -0.0266, -0.1903,  0.0647, -0.0454,  0.0256,\n",
       "          -0.1873,  0.1608, -0.1900,  0.1361],\n",
       "         [ 0.0131,  0.1056, -0.1333,  0.0439,  0.1979,  0.0175,  0.1750, -0.1222,\n",
       "          -0.0817, -0.0892,  0.1707, -0.0916, -0.1860, -0.1852, -0.2149, -0.1318,\n",
       "           0.2034,  0.1620,  0.0876, -0.1750]], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.1515, -0.1099,  0.1282, -0.1673,  0.0647,  0.1156,  0.1880,  0.2261,\n",
       "          -0.1938,  0.0718]], requires_grad=True)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can set reinitialize all of the weights to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.]], requires_grad=True), Parameter containing:\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], requires_grad=True)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for w in all_weights:\n",
    "    nn.init.ones_(w)\n",
    "all_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then go back to the original model, we can see that the parameters have indeed been changed!  The init.ones_ method is likely a poor choice for the weights matrix, but there are many others that exist in torch.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([ 0.0389, -0.1127,  0.1706, -0.0756,  0.0622,  0.2146,  0.0049, -0.0633,\n",
       "          0.0138, -0.0113,  0.0814, -0.0326, -0.0885,  0.0271,  0.1004, -0.1497,\n",
       "         -0.1454,  0.1346,  0.0089, -0.2216], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1943,  0.0686, -0.1350,  0.1720,  0.1658,  0.1730, -0.1000,  0.0500,\n",
       "         -0.1416,  0.1595], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1677], requires_grad=True)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn_model.parameters())[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can also set the parameters yourself or use your own custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_to_zero(tensor):\n",
    "    tensor.data = torch.zeros(tensor.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    set_to_zero(all_weights[-3])\n",
    "    #all_weights[-1] = torch.zeros(w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([ 0.0389, -0.1127,  0.1706, -0.0756,  0.0622,  0.2146,  0.0049, -0.0633,\n",
       "          0.0138, -0.0113,  0.0814, -0.0326, -0.0885,  0.0271,  0.1004, -0.1497,\n",
       "         -0.1454,  0.1346,  0.0089, -0.2216], requires_grad=True), Parameter containing:\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.1943,  0.0686, -0.1350,  0.1720,  0.1658,  0.1730, -0.1000,  0.0500,\n",
       "         -0.1416,  0.1595], requires_grad=True), Parameter containing:\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.1677], requires_grad=True)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn_model.parameters())[-6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Using the GPU\n",
    "\n",
    "So far, we have been using the CPU to make predictions using our models.  This is alright for single use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12 s ± 363 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mixed_model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1155, 1.2353, 0.8248, 1.2000, 0.8952, 0.2981, 0.8039, 1.1539, 0.3607,\n",
       "        0.5310, 1.7747, 0.4487, 0.9864, 0.9546, 0.5529, 1.1806],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_model.cuda()\n",
    "xb = tuple(map(lambda x: x.cuda(), xb))\n",
    "yb.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.3 ms ± 1.05 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mixed_model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my machine, we see that the time for CPU inference is 2.12s and the time for GPU inference is 16.3ms.  That means that the GPU is almost 130 times faster for this model, which is why most of deep learning is performed on the GPU!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
