{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "\n",
    "from scripts.dataloader import Dataset, Transforms, Resize, ToTorch, Sampler, collate, DataLoader\n",
    "from scripts.custom_models import Lambda, flatten, MixedInputModel, TabularModel, CNNModel, CustomResnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0 - Importing our data\n",
    "\n",
    "We will be using our custom dataloaders for many of the subsequent installments, so we will create a function to fetch these variables easily moving forward.  The final thing that we need to add to our dataloaders, however, is to add a validation set and dataloader.  We can accomplish this using the 'indices' input from our custom dataset class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_example_dataloaders():\n",
    "    df_path = r'data/processed_dataframe.csv'\n",
    "    img_col = 'filename'\n",
    "    cont_cols = ['followers', 'following', 'engagement_factor_std', 'month', 'year', 'day_name', 'hour']\n",
    "    cat_cols = []\n",
    "    target_col = 'engagement_factor_moving_avg'\n",
    "    image_path = Path(r'data/Images')\n",
    "    tfms = Transforms([Resize(256), ToTorch()])\n",
    "    \n",
    "    df = pd.read_csv(r'data/processed_dataframe.csv')\n",
    "    train_idx = df.sample(frac=0.8).index\n",
    "    valid_idx = df.loc[~df.index.isin(train_idx)].index\n",
    "\n",
    "    ds_train = Dataset(df_path, \n",
    "                       img_col = img_col,\n",
    "                       cont_cols = cont_cols, \n",
    "                       cat_cols = cat_cols, \n",
    "                       target_col = target_col, \n",
    "                       image_path = image_path, \n",
    "                       transforms = tfms,\n",
    "                       indices = train_idx)\n",
    "    \n",
    "    ds_valid = Dataset(df_path, \n",
    "                   img_col = img_col,\n",
    "                   cont_cols = cont_cols, \n",
    "                   cat_cols = cat_cols, \n",
    "                   target_col = target_col, \n",
    "                   image_path = image_path, \n",
    "                   transforms = tfms,\n",
    "                   indices = valid_idx)\n",
    "\n",
    "    dl_train = DataLoader(dataset = ds_train,\n",
    "                          sampler = Sampler(ds_train, bs = 16),\n",
    "                          collate_func = collate)\n",
    "    \n",
    "    dl_valid = DataLoader(dataset = ds_valid,\n",
    "                          sampler = Sampler(ds_valid, bs = 16),\n",
    "                          collate_func = collate)\n",
    "    \n",
    "    \n",
    "    return dl_train, dl_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_valid = get_example_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(dl_train))\n",
    "x_image, x_tab = xb\n",
    "\n",
    "bs, ch_img, h_img, w_img = x_image.shape\n",
    "bs, tab_inputs = x_tab.shape\n",
    "\n",
    "num_cnn_outputs = 10\n",
    "num_tabular_outputs = 4\n",
    "\n",
    "num_mixed_inputs = num_cnn_outputs + num_tabular_outputs\n",
    "\n",
    "\n",
    "input_cnn_model = CustomResnet(torchvision.models.resnet34(pretrained = True), [1000,50,20, num_cnn_outputs])\n",
    "input_tabular_model = TabularModel([tab_inputs, 10, num_tabular_outputs])\n",
    "input_mixed_model = TabularModel([num_mixed_inputs, 7, 1])\n",
    "\n",
    "model = MixedInputModel(input_cnn_model, input_tabular_model, input_mixed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Overview of The Training Loop\n",
    "\n",
    "Training a cycle of a deep learning model requires a set of consistent steps that are fairly data-type agnostic.  These are: \n",
    "1. Load the xb and yb using a **Dataloader**\n",
    "2. Run xb through the **model**\n",
    "3. Assign how differences between the model's predictions and the target data (yb) are calculated as loss using a **loss function**\n",
    "4. Starting with the loss, use a **backpropagation method** to traverse through the network to determine the gradient of each parameter with with respect to the final loss tensor\n",
    "5. Update each parameter accordingly using an **optimizer**\n",
    "6. Reset the gradients of each parameter to prepare for the next cycle\n",
    "\n",
    "We have already created many of the components required, but still require a **loss function**, a **backpropagation method**, and an **optimizer**.  Those will be the subject of todays instalment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, we should take a quick look at the output from our model.  With a batch size of 16 and a single prediction for each item, we are left with a tensor of size (16,1).  Our predictions are of shape (16), so we should make sure to squeeze out the extra dimension from our model before doing any operations between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1]), torch.Size([16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(xb)\n",
    "preds.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(preds, -1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - The Loss Function\n",
    "\n",
    "One of the most critical decision in a deep learning model is choosing how differences between the model's outputs and the true values should be penalized.  This is particularly important because the objective of training is to reduce this loss value to as small a number as possible over many cycles and epochs of training.  Therefore, the loss values calculated by the loss function should ideally be directly correlated with your desired model performance, with a decrease in loss always representing an increase in performance for your purposes.  \n",
    "\n",
    "Our example is a simple regression problem, which means that we have a few options including:\n",
    "- Mean Squared Error\n",
    "- Mean Absolute Error\n",
    "- Mean Squared Logarithmic Error\n",
    "\n",
    "Today, we will be using mean squared error, as it is a safe first choice and is easy to understand.  Mean squared error is defined as: $$\\operatorname{MSE}=\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y_i})^2$$\n",
    "which can be written in python as `(inputs-targets)**2).mean()`.  We can write this as simple function and test the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(inp, target):\n",
    "    if len(inp.shape) > len(target.shape): inp = torch.squeeze(inp)\n",
    "    return ((inp-target)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10470802., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set this up as an nn.Module to fit better into our model's flow.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE_Loss(nn.Module):\n",
    "    def __init__(self, reduce = False):\n",
    "        super(MSE_Loss, self).__init__()\n",
    "        self.reduce = reduce\n",
    "        \n",
    "    def forward(self, inp, target):\n",
    "        if self.reduce: inp = torch.squeeze(inp, -1)\n",
    "        return ((inp-target)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10470802., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = MSE_Loss(reduce = True)\n",
    "loss = loss_func(preds, yb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we can treat the loss as a module like any other.  In fact, we could start the process of backpropagation (discussed next) from any tensor in the network, although it likely wouldn't be useful to improving performance.  The loss function is an excellent way to add customization or use domain knowledge to help your model reach peak performance.  For instance, if you want a segmentation model to segment your object but never go over the object's bound (preferring to undersegment it), you can increase the penalty for labelling the background as the object, thereby encouraging your model to act in your desired manner (this can also be implemented using weighted cross-entropy loss)\n",
    "\n",
    "The loss function is also an easy way to implements certain regularization techniques, such as L1 or L2 regularization so make sure you explore all the low-hanging fruit during your own custom implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to keep things simple, PyTorch also has a wide range of loss functions available to use.  MSE is available from the MSELoss class, which essentially performs the same operation as our module with a few extra options and warnings\n",
    "\n",
    "```python \n",
    "class MSELoss(_Loss):\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(MSELoss, self).__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return F.mse_loss(input, target, reduction=self.reduction)\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def mse_loss(input, target, size_average=None, reduce=None, reduction='mean'):\n",
    "    if not (target.size() == input.size()):\n",
    "        warnings.warn(\"Using a target size ({}) that is different to the input size ({}). \"\n",
    "                      \"This will likely lead to incorrect results due to broadcasting. \"\n",
    "                      \"Please ensure they have the same size.\".format(target.size(), input.size()),\n",
    "                      stacklevel=2)\n",
    "    if size_average is not None or reduce is not None:\n",
    "        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "    if target.requires_grad:\n",
    "        ret = (input - target) ** 2\n",
    "        if reduction != 'none':\n",
    "            ret = torch.mean(ret) if reduction == 'mean' else torch.sum(ret)\n",
    "    else:\n",
    "        expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
    "        ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))\n",
    "    return ret\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10470802., grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_loss_func(torch.squeeze(preds), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the loss calculated by all three methods is identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Backpropagation\n",
    "\n",
    "As the goal of training is to minimize loss, there needs to be a method of calculating how each parameter contributes to an increase in loss.  In other words, the gradient of that parameter with respect to the loss.  This would be an exceptionally complex calculation to do in a single step, but can be broken down into simple steps by using the chain rule and tracing the gradients back one by one from the loss to the parameter of interest.  How deeply you want to get in your understanding of this process is up to you, but fortunately PyTorch has an autograd feature that tracks the operations of each tensor and can then be used to calculate all of these gradients automatically.  To start the process, choose the starting tensor of choice (in this case the output from the loss function) and call the class method `.backward()`\n",
    "\n",
    "Note: After calculating the gradients, the buffers keeping track of the operations are freed.  Running backwards twice in a row will results in a `RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a look at some of the parameters by examing their gradients.  As we can see, the shapes of the grads and parameters are identical: there is one gradient for every element of each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.2824, -0.2682,  0.2561, -0.3252,  0.1252, -0.1312,  0.1911]],\n",
       "        requires_grad=True),\n",
       " tensor([[42169308., 35757724., 46602248.,        0., 81614776., 26811706.,\n",
       "                 0.]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "params[-2], params[-2].grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that the gradients in this example are either very high or very low (exploding or vanishing gradients).  This is something that we hope to avoid through proper initialization and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - The Optimizer (Updating the Parameters)\n",
    "\n",
    "Once we have the gradients calculated, we know that by moving each parameter in the direction of the gradient, it will act to increase the loss.  Instead, we want to move each parameter in the opposite direction proportional to the intensity of the gradient.  The easiest way to do that is to choose a scaling factor (the learning rate), then subtract the (learning rate * gradient) from each parameter.  \n",
    "\n",
    "As before, we can access all of the parameters in the model using the `.parameters()` class method, an ability imparted by the nn.Module base class.  We can then iterate through all the parameters and subtract the negative gradient * the learning rate.  It's important to note that we need to use the context of torch.no_grad(), since we don't want these operations to be tracked by the autograd function.  \n",
    "\n",
    "When we're finished, we have to set all of the gradients to zero to prepare them for the next cycle of training.  The reason that we don't automatically do it with the step operation is to provide more flexibility to training.  For instance, if you have limited GPU RAM, but would like to train with large batch sizes, one option is to run the multiple training cycles without either updating the parameters or zeroing the gradients.  This allows the gradients to accumulate, so multiple cycles can be averaged in a single update pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.2824, -0.2682,  0.2561, -0.3252,  0.1252, -0.1312,  0.1911]],\n",
       "        requires_grad=True),\n",
       " tensor([[42169308., 35757724., 46602248.,        0., 81614776., 26811706.,\n",
       "                 0.]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "params[-2], params[-2].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, model, lr):\n",
    "        self.params = list(model.parameters())\n",
    "        self.lr = lr   #learning rate\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                p -= self.lr*p.grad\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.data.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model, lr = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-8.4339e+05, -7.1515e+05, -9.3204e+05, -3.2516e-01, -1.6323e+06,\n",
       "          -5.3623e+05,  1.9110e-01]], requires_grad=True),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "params[-2], params[-2].grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after the calls to step and zero_grad, the values of our parameters have changed and the gradients have been zeroed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Putting it all together\n",
    "\n",
    "We now have all the tools required to train our model using all of the batches from our training set.  Passing through our training set in this manner is called an 'epoch'.  The goal now is just to run through all of the steps in sequential order.\n",
    "\n",
    "Note: we will handle moving all the components onto the GPU later, but for now, we will use one additional function to make the process easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\n",
      " Validation Step:\n",
      "tensor(nan, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "\n",
    "def move_data_to_GPU(xb, yb):\n",
    "    xb = tuple(map(lambda x: x.cuda(), xb))\n",
    "    yb = yb.cuda()\n",
    "    return xb, yb\n",
    "\n",
    "\n",
    "\n",
    "def epoch(dl_train, dl_valid, model, loss_func, opt, early_stop = None):\n",
    "    #Training Pass\n",
    "    i = 0\n",
    "    for xb, yb in dl_train:\n",
    "        #Temporary code\n",
    "        xb, yb = move_data_to_GPU(xb,yb)\n",
    "\n",
    "        #Actual Train Cycle\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        print(loss)\n",
    "        i+= 1\n",
    "        if early_stop is not None and i>early_stop: break\n",
    "    \n",
    "    print('\\n Validation Step:')\n",
    "    val_batches = 0\n",
    "    tot_loss = 0\n",
    "    i = 0\n",
    "    for xb, yb in dl_valid:\n",
    "        #Temporary code\n",
    "        xb, yb = move_data_to_GPU(xb,yb)\n",
    "        with torch.no_grad():\n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            tot_loss += loss\n",
    "            val_batches+= 1\n",
    "            \n",
    "            i += 1\n",
    "            if early_stop is not None and i>early_stop: break\n",
    "            \n",
    "    print(tot_loss/val_batches)\n",
    "        \n",
    "epoch(dl_train, dl_valid,model, loss_func, opt, early_stop = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We're able to train, but we can see that after the first iteration, the gradients explode.  This is not surprising given everything that we've thrown together.  Training a complex deep learning model is no easy feat!  In the next few iterations, we will set up a callback system to make things more flexible, as well as investigate other optimizations such as learning rate, initialization and tuning hyperparameters.  Using the model building blocks (cnn_model, mixed_model, tab_model), we can see how we can create new models that share the internal components.  This allows us to train components of the model seperately, then combine everything together once the network is fairly stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
