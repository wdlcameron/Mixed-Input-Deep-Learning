{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import imageio\n",
    "from skimage import io, transform\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Creating your dataset\n",
    "\n",
    "The first step towards creating a dataloader is to create a dataset so that you pass in an index to get an item in the desired form.  \n",
    "\n",
    "To understand the basic principles behind the dataset class, we will start with the most basic components.  Our simple dataset class takes in a subscriptable list for both the inputs (x) and outputs (y) and produces an tuple of the output (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        assert len(x) == len(y), 'Size mismatched between inputs and labels'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0,1,2,3,4,5,6,7,8,9]\n",
    "y = [5,3,2,6,7,8,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_simple = SimpleDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element: (0, 5)\n",
      "Length: 10\n"
     ]
    }
   ],
   "source": [
    "print (\"First element:\", ds_train_simple[0])\n",
    "print(\"Length:\", len(ds_train_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our application, we want our dataset to output a tuple containing two elements:  \n",
    "1. The inputs, which will be a tuple of a the image and the tabular data\n",
    "2. The labels\n",
    "\n",
    "Before entering the model for training, all inputs need to be converted into torch tensors.  We have chosen to do it now, but it could also have been done at a later stage.  Similarly, image transformations can happen at any time before they are stacked together and are fed into the model.  We will perform our transformations here for simplicity.\n",
    "\n",
    "Note: the Dataset class has been written such that elements are accessed one at a time.  This is by design as it allows you more flexibility during the collate phase; however, it is possible to restructure the __getitem__ method to process multiple indices.  The major change would be to iterate through the filenames, then open and process each individual file before stacking them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    DROP_WARNING = 0.9\n",
    "    \n",
    "    def __init__(self, df_path, img_col, cont_cols, cat_cols, target_col, image_path, suffix = '.jpg', transforms = None):\n",
    "        self.df_path = df_path\n",
    "        self.img_col, self.cont_cols, self.cat_cols = img_col, cont_cols, cat_cols\n",
    "        self.target_col = target_col\n",
    "        self.suffix = suffix\n",
    "        self.image_path = Path(image_path)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        #read in the dataframe\n",
    "        self.df = pd.read_csv(df_path)\n",
    "        self.df = self.clean_dataframe(self.df)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def clean_dataframe(self, df):\n",
    "\n",
    "        orig_len = len(df)\n",
    "        \n",
    "        #Remove filenames for files that do not exist (or have errors in the filepath)\n",
    "        existing_files = ((pd.Series([self.image_path]*len(df))/(df[image_col]+'.jpg'))\n",
    "                          .apply(lambda x: self.check_path_valid(x)))\n",
    "        \n",
    "        df.drop(df[~existing_files].index, axis = 0, inplace = True)\n",
    "        \n",
    "        #Remove missing values from your target columns\n",
    "        df.drop(df[df[self.target_col].isna()].index, axis = 0, inplace = True)\n",
    "\n",
    "        \n",
    "        df.reset_index(drop=True, inplace = True)\n",
    "        \n",
    "        if len(df)/orig_len < self.DROP_WARNING: \n",
    "            print (f\"Warning, more than {(1-self.DROP_WARNING)*100}% of your data was invalid\")\n",
    "        return df \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def check_path_valid(self, path):\n",
    "        try: return path.exists()\n",
    "        except: return False\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.df.loc[idx, self.img_col]\n",
    "        cat_data = self.df.loc[idx, self.cat_cols]\n",
    "        cont_data = self.df.loc[idx, self.cont_cols].values.astype(np.float32)\n",
    "        target = self.df.loc[idx, self.target_col]\n",
    "        \n",
    "        tabular_data = torch.tensor(cont_data)\n",
    "        target = torch.tensor(target)\n",
    "        \n",
    "        image = io.imread(self.image_path/(filename + self.suffix))\n",
    "        \n",
    "        if self.transforms: image = self.transforms(image)\n",
    "            \n",
    "        \n",
    "        return (image, tabular_data), target        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b522593a7b41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                    \u001b[0mtarget_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                    \u001b[0mimage_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                    transforms = None)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-453668d65c11>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, df_path, img_col, cont_cols, cat_cols, target_col, image_path, suffix, transforms)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#read in the dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-453668d65c11>\u001b[0m in \u001b[0;36mclean_dataframe\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#Remove filenames for files that do not exist (or have errors in the filepath)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         existing_files = ((pd.Series([self.image_path]*len(df))/(df[image_col]+'.jpg'))\n\u001b[0m\u001b[0;32m     28\u001b[0m                           .apply(lambda x: self.check_path_valid(x)))\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_col' is not defined"
     ]
    }
   ],
   "source": [
    "df_path = r'data/processed_dataframe.csv'\n",
    "img_col = 'filename'\n",
    "cont_cols = ['followers', 'following', 'engagement_factor_std', 'month', 'year', 'day_name', 'hour']\n",
    "cat_cols = []\n",
    "target_col = 'engagement_factor_moving_avg'\n",
    "image_path = Path(r'data/Images')\n",
    "\n",
    "ds_train = Dataset(df_path, \n",
    "                   img_col = img_col,\n",
    "                   cont_cols = cont_cols, \n",
    "                   cat_cols = cat_cols, \n",
    "                   target_col = target_col, \n",
    "                   image_path = image_path, \n",
    "                   transforms = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length:\", len(ds_train), '\\n')\n",
    "\n",
    "print('First component:\\n', ds_train[0][0][0], ds_train[0][0][0].shape, '\\n')\n",
    "print('Second component:\\n', ds_train[0][0][1], '\\n')\n",
    "print('Third component:\\n', ds_train[0][1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous example, we did not include any transforms.  This will be an issue moving forward for a few reasons:\n",
    "1. We did not convert our image into a torch tensor\n",
    "2. There will be size mismatches across images that will make it difficult to stack\n",
    "3. Pytorch expects the images to have dimension (c, h, w), but our image arrays are arranged as (h, w, c)\n",
    "\n",
    "We can create a Transforms class, which will call a series of transforms in sequence.  The most basic transforms required to solve the above issues are a Resize transform (in which we'll rely on skimage to perform the resize), and a ToTorch() transform, which will rearrange the channels and convert the array to a torch tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transforms():\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for tsfm in self.transforms:\n",
    "            x = tsfm(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Resize():\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        return transform.resize(img, (self.size, self.size))\n",
    "    \n",
    "    \n",
    "class ToTorch():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        return torch.tensor(img.transpose(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = r'data/processed_dataframe.csv'\n",
    "img_col = 'filename'\n",
    "cont_cols = ['followers', 'following', 'engagement_factor_std', 'month', 'year', 'day_name', 'hour']\n",
    "cat_cols = []\n",
    "target_col = 'engagement_factor_moving_avg'\n",
    "image_path = Path(r'data/Images')\n",
    "tfms = Transforms([Resize(255), ToTorch()])\n",
    "\n",
    "ds_train = Dataset(df_path, \n",
    "                   img_col = img_col,\n",
    "                   cont_cols = cont_cols, \n",
    "                   cat_cols = cat_cols, \n",
    "                   target_col = target_col, \n",
    "                   image_path = image_path, \n",
    "                   transforms = tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length:\", len(ds_train), '\\n')\n",
    "\n",
    "print('First component:\\n', ds_train[0][0][0], ds_train[0][0][0].shape, '\\n')\n",
    "print('Second component:\\n', ds_train[0][0][1], '\\n')\n",
    "print('Third component:\\n', ds_train[0][1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that our image is a tensor.  As a side effect of the resize, the values have now been scaled to the range 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Data Sampler\n",
    "\n",
    "Now that we have a method of accessing individual elements from our dataset, we now need a method of arranging them into minibatches for training.  This will be accomplished by choosing the indices for each batch.  The simplest method of doing this is choosing a batch size, then dividing the data into blocks of that size (with the last block containing whatever is left over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasySampler():\n",
    "    def __init__(self, dataset, bs):\n",
    "        self.dataset = dataset\n",
    "        self.bs = bs #batch_size\n",
    "        \n",
    "        self.n = len(dataset)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range((self.n-1)//self.bs + 1):\n",
    "            yield self.dataset[i*self.bs:(i+1)*self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_sampler = EasySampler([0,3,4,6,4,5,65,4, 8, 22], bs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in simple_sampler:\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach works, but has a significant drawback.  Each of the minibatchs will have the same elements across each epoch of training.  Since loss is pooled across the entire minibatch (and sometimes more than one!), this limits the range of inputs it is effectively learning from.  Instead, it would be better to shuffle the data before grouping them into minibatches.  For perspective, for a dataset with 16 unique elements and a batch size of 3, you will have 4 unique minibatch combinations.  When shuffling, this number increases to 560 unique minibatch combinations.\n",
    "\n",
    "There are many ways to shuffle the data, but a simple approach is to create an array of all the possible indices of our dataset, shuffle them and then group the shuffled indices as we did with the Easy Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, dataset, bs, shuffle = True):\n",
    "        self.dataset = dataset\n",
    "        self.bs = bs\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.n = len(self.dataset)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        #idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        idxs = np.random.permutation(self.n) if self.shuffle else np.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs):\n",
    "            yield idxs[i:i+self.bs]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(ds_train, bs = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, idxs in enumerate(sampler):\n",
    "    print(idxs)\n",
    "    if i>5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a sampler that provides random arrangements of our data.  Run the above cell a few times to confirm that the numbers change each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Collate our items\n",
    "\n",
    "The last thing we need to add is a method of assembling the individual items from each minibatch into the x and y batches we use to feed to the model.  Here, we want an output that is of the form xb, yb.  That means we will pack together both the image and tabular data into one variable.  However, we also want an easy method of splitting up the image and tabular stacks apart inside the model.  We can accomplish this by creating individual stacks of each data type, then creating two tuples: one of the tabular and image stacks (the xs tuple), the other of the xs tuple and the ys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data, transforms = None):\n",
    "    xs, y = zip(*data)\n",
    "    x1, x2 = zip(*xs)\n",
    "    \n",
    "    if transforms: x1 = transforms(x1)\n",
    "    \n",
    "    return (torch.stack(x1), torch.stack(x2)), torch.stack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "minibatch_samples = [ds_train[x] for x in range(bs)]\n",
    "minibatch = collate(minibatch_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = minibatch\n",
    "x1, x2 = xs\n",
    "\n",
    "print(f\"Based on a minibatch of size {bs}:\", '\\n')\n",
    "\n",
    "print (f\"The shape of x1 is: {x1.shape}\")\n",
    "print (f\"The shape of x2 is: {x2.shape}\")\n",
    "print (f\"The shape of y  is: {ys.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it.  We now have a method of arranging our data into minibatches for direct input into the training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - DataLoader (Putting it all together)\n",
    "\n",
    "We now have all of the components required to create our dataloader.  At this point, it's really just about putting all the pieces together.  We will input our dataset, sample and collate function into the DataLoader class, then create an iterator that goes through the minibatch indices output by the sampler.  From those, we can then access the relevant components from the dataset and assemble them into the minibatch using the collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, sampler, collate_func):\n",
    "        self.dataset = dataset\n",
    "        self.sampler = sampler\n",
    "        self.collate_func = collate_func\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for idxs in self.sampler: \n",
    "            minibatch = [self.dataset[idx] for idx in idxs]\n",
    "            yield (self.collate_func(minibatch))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(dataset = ds_train,\n",
    "                      sampler = Sampler(ds_train, bs = 16),\n",
    "                      collate_func = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (xb,yb) in enumerate(dl_train):\n",
    "    print (f\"Minibatch {i}, with target shape {yb.shape}\")\n",
    "    if i>5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notes: \n",
    "\n",
    "There were a couple of things that we left out of this process.  Notable was the omission of categorical variables and how to process them.  That requires a more sophisticated look into the model itself, but for now we won't worry about that.  If you try and use categorical variables with the current implementation, it will fail because strings cannot be converted into tensors directly.  For this to work, you have to assign each category a unique number and then convert the categorical variables into their numerical equivalent  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
