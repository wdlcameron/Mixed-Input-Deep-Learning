{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import imageio\n",
    "from skimage import io, transform\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Creating your dataset\n",
    "\n",
    "The first step towards creating a dataloader is to create a dataset so that you pass in an index to get an item in the desired form.  \n",
    "\n",
    "To understand the basic principles behind the dataset class, we will start with the most basic components.  Our simple dataset class takes in a subscriptable list for both the inputs (x) and outputs (y) and produces an tuple of the output (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        assert len(x) == len(y), 'Size mismatched between inputs and labels'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0,1,2,3,4,5,6,7,8,9]\n",
    "y = [5,3,2,6,7,8,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_simple = SimpleDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element: (0, 5)\n",
      "Length: 10\n"
     ]
    }
   ],
   "source": [
    "print (\"First element:\", ds_train_simple[0])\n",
    "print(\"Length:\", len(ds_train_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our application, we want our dataset to output a tuple containing two elements:  \n",
    "1. The inputs, which will be a tuple of the image and the tabular data\n",
    "2. The labels\n",
    "\n",
    "Before entering the model for training, all inputs need to be converted into torch tensors.  We have chosen to do it now, but it could also have been done at a later stage.  Similarly, image transformations can happen at any time before they are stacked together and are fed into the model.  We will perform our transformations here for simplicity.\n",
    "\n",
    "Note: the Dataset class has been written such that elements are accessed one at a time.  This is by design as it allows you more flexibility during the collate phase; however, it is possible to restructure the __getitem__ method to process multiple indices.  The major change would be to iterate through the filenames, then open and process each individual file before stacking them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset():\n",
    "    DROP_WARNING = 0.9\n",
    "    \n",
    "    def __init__(self, df_path, img_col, cont_cols, cat_cols, target_col, image_path, suffix = '.jpg', transforms = None):\n",
    "        self.df_path = df_path\n",
    "        self.img_col, self.cont_cols, self.cat_cols = img_col, cont_cols, cat_cols\n",
    "        self.target_col = target_col\n",
    "        self.suffix = suffix\n",
    "        self.image_path = Path(image_path)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        #read in the dataframe\n",
    "        self.df = pd.read_csv(df_path)\n",
    "        self.df = self.clean_dataframe(self.df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def clean_dataframe(self, df):\n",
    "        orig_len = len(df)\n",
    "        \n",
    "        #Remove filenames for files that do not exist (or have errors in the filepath)\n",
    "        existing_files = ((pd.Series([self.image_path]*len(df))/(df[self.img_col]+'.jpg'))\n",
    "                          .apply(lambda x: self.check_path_valid(x)))\n",
    "        \n",
    "        df.drop(df[~existing_files].index, axis = 0, inplace = True)\n",
    "        \n",
    "        #Remove missing values from your target columns\n",
    "        df.drop(df[df[self.target_col].isna()].index, axis = 0, inplace = True)\n",
    "\n",
    "        df.reset_index(drop=True, inplace = True)\n",
    "        if len(df)/orig_len < self.DROP_WARNING: \n",
    "            print (f\"Warning, more than {(1-self.DROP_WARNING)*100}% of your data was invalid\")\n",
    "        return df \n",
    "        \n",
    "    def check_path_valid(self, path):\n",
    "        try: return path.exists()\n",
    "        except: return False\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.df.loc[idx, self.img_col]\n",
    "        cat_data = self.df.loc[idx, self.cat_cols]\n",
    "        cont_data = self.df.loc[idx, self.cont_cols].values.astype(np.float32)\n",
    "        target = self.df.loc[idx, self.target_col]\n",
    "        \n",
    "        tabular_data = torch.tensor(cont_data)\n",
    "        target = torch.tensor(target)\n",
    "        \n",
    "        image = io.imread(self.image_path/(filename + self.suffix))\n",
    "        if self.transforms: image = self.transforms(image)\n",
    "            \n",
    "        return (image, tabular_data), target        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = r'data/processed_dataframe.csv'\n",
    "img_col = 'filename'\n",
    "cont_cols = ['followers', 'following', 'engagement_factor_std', 'month', 'year', 'day_name', 'hour']\n",
    "cat_cols = []\n",
    "target_col = 'engagement_factor_moving_avg'\n",
    "image_path = Path(r'data/Images')\n",
    "\n",
    "ds_train = Dataset(df_path, \n",
    "                   img_col = img_col,\n",
    "                   cont_cols = cont_cols, \n",
    "                   cat_cols = cat_cols, \n",
    "                   target_col = target_col, \n",
    "                   image_path = image_path, \n",
    "                   transforms = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 13775 \n",
      "\n",
      "First component:\n",
      " [[[237 236 242]\n",
      "  [237 236 242]\n",
      "  [237 236 242]\n",
      "  ...\n",
      "  [205 223 233]\n",
      "  [213 231 241]\n",
      "  [200 218 228]]\n",
      "\n",
      " [[232 231 237]\n",
      "  [233 232 238]\n",
      "  [233 232 238]\n",
      "  ...\n",
      "  [203 221 231]\n",
      "  [203 221 231]\n",
      "  [206 224 234]]\n",
      "\n",
      " [[230 229 235]\n",
      "  [231 230 236]\n",
      "  [232 231 237]\n",
      "  ...\n",
      "  [208 226 236]\n",
      "  [203 221 231]\n",
      "  [195 213 223]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[226 224 237]\n",
      "  [224 222 235]\n",
      "  [223 221 234]\n",
      "  ...\n",
      "  [186 182 196]\n",
      "  [184 180 194]\n",
      "  [185 181 195]]\n",
      "\n",
      " [[226 224 237]\n",
      "  [224 222 235]\n",
      "  [223 221 234]\n",
      "  ...\n",
      "  [188 184 198]\n",
      "  [186 182 196]\n",
      "  [188 184 198]]\n",
      "\n",
      " [[226 224 237]\n",
      "  [224 222 235]\n",
      "  [223 221 234]\n",
      "  ...\n",
      "  [189 185 199]\n",
      "  [188 184 198]\n",
      "  [191 187 201]]] (1347, 1080, 3) \n",
      "\n",
      "Second component:\n",
      " tensor([1.2927e+05, 1.3410e+03, 5.1537e-01, 1.2000e+01, 2.0190e+03, 7.0000e+00,\n",
      "        2.2000e+01]) \n",
      "\n",
      "Third component:\n",
      " tensor(1.4480) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Length:\", len(ds_train), '\\n')\n",
    "\n",
    "print('First component:\\n', ds_train[0][0][0], ds_train[0][0][0].shape, '\\n')\n",
    "print('Second component:\\n', ds_train[0][0][1], '\\n')\n",
    "print('Third component:\\n', ds_train[0][1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous example, we did not include any transforms.  This will be an issue moving forward for a few reasons:\n",
    "1. We did not convert our image into a torch tensor\n",
    "2. There will be size mismatches across images that will make it difficult to stack\n",
    "3. Pytorch expects the images to have dimension (c, h, w), but our image arrays are arranged as (h, w, c)\n",
    "\n",
    "We can create a Transforms class, which will call a series of transforms in sequence.  The most basic transforms required to solve the above issues are a Resize transform (in which we'll rely on skimage to perform the resize), and a ToTorch() transform, which will rearrange the channels and convert the array to a torch tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Transforms():\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for tsfm in self.transforms:\n",
    "            x = tsfm(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Resize():\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        return transform.resize(img, (self.size, self.size))\n",
    "    \n",
    "    \n",
    "class ToTorch():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        return torch.tensor(img.transpose(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = r'data/processed_dataframe.csv'\n",
    "img_col = 'filename'\n",
    "cont_cols = ['followers', 'following', 'engagement_factor_std', 'month', 'year', 'day_name', 'hour']\n",
    "cat_cols = []\n",
    "target_col = 'engagement_factor_moving_avg'\n",
    "image_path = Path(r'data/Images')\n",
    "tfms = Transforms([Resize(255), ToTorch()])\n",
    "\n",
    "ds_train = Dataset(df_path, \n",
    "                   img_col = img_col,\n",
    "                   cont_cols = cont_cols, \n",
    "                   cat_cols = cat_cols, \n",
    "                   target_col = target_col, \n",
    "                   image_path = image_path, \n",
    "                   transforms = tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 13775 \n",
      "\n",
      "First component:\n",
      " tensor([[[0.9098, 0.9059, 0.9020,  ..., 0.7947, 0.7990, 0.8006],\n",
      "         [0.9098, 0.9059, 0.9100,  ..., 0.8171, 0.8020, 0.8015],\n",
      "         [0.9098, 0.9122, 0.9140,  ..., 0.7987, 0.8016, 0.7966],\n",
      "         ...,\n",
      "         [0.8588, 0.8598, 0.8667,  ..., 0.8003, 0.7765, 0.7503],\n",
      "         [0.8641, 0.8689, 0.8706,  ..., 0.8003, 0.7756, 0.7427],\n",
      "         [0.8745, 0.8745, 0.8745,  ..., 0.7998, 0.7788, 0.7397]],\n",
      "\n",
      "        [[0.9059, 0.9020, 0.8980,  ..., 0.8613, 0.8662, 0.8711],\n",
      "         [0.9059, 0.9020, 0.9060,  ..., 0.8838, 0.8687, 0.8721],\n",
      "         [0.9059, 0.9082, 0.9126,  ..., 0.8682, 0.8683, 0.8672],\n",
      "         ...,\n",
      "         [0.8510, 0.8520, 0.8588,  ..., 0.7847, 0.7557, 0.7268],\n",
      "         [0.8563, 0.8611, 0.8627,  ..., 0.7847, 0.7557, 0.7192],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.7842, 0.7591, 0.7201]],\n",
      "\n",
      "        [[0.9294, 0.9255, 0.9216,  ..., 0.9006, 0.9054, 0.9104],\n",
      "         [0.9294, 0.9255, 0.9296,  ..., 0.9230, 0.9079, 0.9113],\n",
      "         [0.9294, 0.9318, 0.9361,  ..., 0.9074, 0.9075, 0.9064],\n",
      "         ...,\n",
      "         [0.9020, 0.9029, 0.9098,  ..., 0.8396, 0.8106, 0.7817],\n",
      "         [0.9073, 0.9121, 0.9137,  ..., 0.8396, 0.8106, 0.7741],\n",
      "         [0.9176, 0.9176, 0.9176,  ..., 0.8391, 0.8140, 0.7750]]],\n",
      "       dtype=torch.float64) torch.Size([3, 255, 255]) \n",
      "\n",
      "Second component:\n",
      " tensor([1.2927e+05, 1.3410e+03, 5.1537e-01, 1.2000e+01, 2.0190e+03, 7.0000e+00,\n",
      "        2.2000e+01]) \n",
      "\n",
      "Third component:\n",
      " tensor(1.4480) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Length:\", len(ds_train), '\\n')\n",
    "\n",
    "print('First component:\\n', ds_train[0][0][0], ds_train[0][0][0].shape, '\\n')\n",
    "print('Second component:\\n', ds_train[0][0][1], '\\n')\n",
    "print('Third component:\\n', ds_train[0][1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that our image is a tensor.  As a side effect of the resize, the values have now been scaled to the range 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Data Sampler\n",
    "\n",
    "Now that we have a method of accessing individual elements from our dataset, we now need a method of arranging them into minibatches for training.  This will be accomplished by choosing the indices for each batch.  The simplest method of doing this is choosing a batch size, then dividing the data into blocks of that size (with the last block containing whatever is left over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasySampler():\n",
    "    def __init__(self, dataset, bs):\n",
    "        self.dataset = dataset\n",
    "        self.bs = bs #batch_size\n",
    "        \n",
    "        self.n = len(dataset)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range((self.n-1)//self.bs + 1):\n",
    "            yield self.dataset[i*self.bs:(i+1)*self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_sampler = EasySampler([0,3,4,6,4,5,65,4, 8, 22], bs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 4]\n",
      "[6, 4, 5]\n",
      "[65, 4, 8]\n",
      "[22]\n"
     ]
    }
   ],
   "source": [
    "for i in simple_sampler:\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach works, but has a significant drawback.  Each of the minibatchs will have the same elements across each epoch of training.  Since loss is pooled across the entire minibatch (and sometimes more than one!), this limits the range of inputs it is effectively learning from.  Instead, it would be better to shuffle the data before grouping them into minibatches.  For perspective, for a dataset with 16 unique elements and a batch size of 3, you will have 4 unique minibatch combinations.  When shuffling, this number increases to 560 unique minibatch combinations.\n",
    "\n",
    "There are many ways to shuffle the data, but a simple approach is to create an array of all the possible indices of our dataset, shuffle them and then group the shuffled indices as we did with the Easy Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Sampler():\n",
    "    def __init__(self, dataset, bs, shuffle = True):\n",
    "        self.dataset = dataset\n",
    "        self.bs = bs\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.n = len(self.dataset)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        #idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        idxs = np.random.permutation(self.n) if self.shuffle else np.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs):\n",
    "            yield idxs[i:i+self.bs]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(ds_train, bs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: [   55  8790  6216  9953   205  8816 11309 11210]\n",
      "Batch 1: [ 5478 12825 10602  1251 13568 10500   777  6385]\n",
      "Batch 2: [ 2245 13113  5805  2662  4619  7439 13329  6526]\n",
      "Batch 3: [ 5068  2217  6017  8544 10194  5581  3033 12668]\n",
      "Batch 4: [ 2319   606  9099 11460  8365 11501 12583  9090]\n",
      "Batch 5: [ 5875 10296  9852  3097  2053 12459   989 13186]\n",
      "Batch 6: [ 4332  1797 12769  7896  6597  1991  5509 12536]\n"
     ]
    }
   ],
   "source": [
    "for i, idxs in enumerate(sampler):\n",
    "    print(f'Batch {i}: {idxs}')\n",
    "    if i>5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a sampler that provides random arrangements of our data.  Run the above cell a few times to confirm that the numbers change each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Collate our items\n",
    "\n",
    "The last thing we need to add is a method of assembling the individual items from each minibatch into the x and y batches we use to feed to the model.  Here, we want an output that is of the form xb, yb.  That means we will pack together both the image and tabular data into one variable.  However, we also want an easy method of splitting up the image and tabular stacks apart inside the model.  We can accomplish this by creating individual stacks of each data type, then creating two tuples: one of the tabular and image stacks (the xs tuple), the other of the xs tuple and the ys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def collate(data, transforms = None):\n",
    "    xs, y = zip(*data)\n",
    "    x1, x2 = zip(*xs)\n",
    "    \n",
    "    if transforms: x1 = transforms(x1)\n",
    "    \n",
    "    return (torch.stack(x1), torch.stack(x2)), torch.stack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "minibatch_samples = [ds_train[x] for x in range(bs)]\n",
    "minibatch = collate(minibatch_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on a minibatch of size 16: \n",
      "\n",
      "The shape of x1 is: torch.Size([16, 3, 255, 255])\n",
      "The shape of x2 is: torch.Size([16, 7])\n",
      "The shape of y  is: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "xs, ys = minibatch\n",
    "x1, x2 = xs\n",
    "\n",
    "print(f\"Based on a minibatch of size {bs}:\", '\\n')\n",
    "\n",
    "print (f\"The shape of x1 is: {x1.shape}\")\n",
    "print (f\"The shape of x2 is: {x2.shape}\")\n",
    "print (f\"The shape of y  is: {ys.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it.  We now have a method of arranging our data into minibatches for direct input into the training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - DataLoader (Putting it all together)\n",
    "\n",
    "We now have all of the components required to create our dataloader.  At this point, it's really just about putting all the pieces together.  We will input our dataset, sample and collate function into the DataLoader class, then create an iterator that goes through the minibatch indices output by the sampler.  From those, we can then access the relevant components from the dataset and assemble them into the minibatch using the collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DataLoader():\n",
    "    def __init__(self, dataset, sampler, collate_func):\n",
    "        self.dataset = dataset\n",
    "        self.sampler = sampler\n",
    "        self.collate_func = collate_func\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for idxs in self.sampler: \n",
    "            minibatch = [self.dataset[idx] for idx in idxs]\n",
    "            yield (self.collate_func(minibatch))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(dataset = ds_train,\n",
    "                      sampler = Sampler(ds_train, bs = 16),\n",
    "                      collate_func = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch 0, with target shape torch.Size([16])\n",
      "Minibatch 1, with target shape torch.Size([16])\n",
      "Minibatch 2, with target shape torch.Size([16])\n",
      "Minibatch 3, with target shape torch.Size([16])\n",
      "Minibatch 4, with target shape torch.Size([16])\n",
      "Minibatch 5, with target shape torch.Size([16])\n",
      "Minibatch 6, with target shape torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for i, (xb,yb) in enumerate(dl_train):\n",
    "    print (f\"Minibatch {i}, with target shape {yb.shape}\")\n",
    "    if i>5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notes: \n",
    "\n",
    "There were a couple of things that we left out of this process.  Notable was the omission of categorical variables and how to process them.  That requires a more sophisticated look into the model itself, but for now we won't worry about that.  If you try and use categorical variables with the current implementation, it will fail because strings cannot be converted into tensors directly.  For this to work, you have to assign each category a unique number and then convert the categorical variables into their numerical equivalent  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Part 1 - Dataloader.ipynb to scripts\\dataloader.py\n"
     ]
    }
   ],
   "source": [
    "!python scripts/notebook2script.py \"Part 1 - Dataloader.ipynb\" 'dataloader.py'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
