
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/Part 4 - Callbacks.ipynb

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
from functools import partial
from pathlib import Path
import torchvision

import math
import matplotlib.pyplot as plt
import re


from scripts.dataloader import Dataset, Transforms, Resize, ToTorch, Sampler, collate, DataLoader
from scripts.custom_models import Lambda, flatten, MixedInputModel, TabularModel, CNNModel, CustomResnet
from scripts.optimizer_and_loss import Optimizer, MSE_Loss, get_example_dataloaders, get_example_model

class Callback():

    _order = 0 #This allows you to control the order callbacks are called in
    def __init__(self): self.learn = None
    def set_learner(self,learn): self.learn = learn
    def __getattr__(self, attr): return getattr(self.learn, attr)  #Will look here if it can't find it in the current class

    def __call__(self, callback):
        f = getattr(self, callback, None)
        if f and f(): return True   #f() won't be called if f is None
        else: return False

    @property
    def name(self):
        name = re.sub(r'Callback$', '', self.__class__.__name__)
        return camel2snake(name or 'callback')



_camel_re1 = re.compile('(.)([A-Z][a-z]+)')
_camel_re2 = re.compile('([a-z0-9])([A-Z])')
def camel2snake(name):
    s1 = re.sub(_camel_re1, r'\1_\2', name)
    return re.sub(_camel_re2, r'\1_\2', s1).lower()

class Datablock():
    def __init__(self, train_dl, valid_dl = None, test_dl = None):
        self.train_dl, self.valid_dl, self.test_dl = train_dl, valid_dl, test_dl



class PreLearner():
    def __init__(self, model, data, loss_func, opt, cbs, cbfs):
        self.model = model
        self.loss_func = loss_func
        self.opt = opt
        self.data = data

        self.callbacks = []
        self.add_callbacks(callbacks)
        self.add_callbacks([cbf() for cbf in callback_functions])

    def add_callbacks(self, cbs):
        for cb in cbs:
            cb.set_learner(self)
            self.callbacks.append(cb)

    def one_batch(self, xb, yb):
        try:
            self.xb, self.yb = xb, yb;                          self('begin_batch')
            self.preds = self.model(self.xb);                   self('after_pred')
            self.loss = self.loss_func(self.preds, self.yb);    self('after_loss')
            if training:
                self.loss.backward();                           self('after_loss')
                self.opt.step();                                self('after_step')
                self.opt.zero_grad()

        except CancelBatchException:                            self('after_cancel_batch')

        finally:                                                self('after_batch')


    def __call__(self, callback_name):
        result = False
        for cb in sorted(self.callbacks, key = lambda x: x._order): result = cb(callback_name) and result
        return result


class Learner():
    def __init__(self, model, data, loss_func, opt, cbs, cbfs):
        self.model = model
        self.loss_func = loss_func
        self.opt = opt
        self.data = data

        self.callbacks = []
        self.add_callbacks([TrainEvalCallback()] + cbs)
        self.add_callbacks([cbf() for cbf in cbfs])

    def add_callbacks(self, cbs):
        for cb in cbs:
            cb.set_learner(self)
            setattr(self, cb.name, cb)
            self.callbacks.append(cb)

    def fit(self, num_epochs):
        self.num_epochs = num_epochs
        try:
            self.loss = torch.tensor(0.);                       self('begin_fit')
            for epoch in range(num_epochs):
                self.epoch = epoch
                if not self('begin_epoch'): #Will return False unless you want to skip the epoch
                    #train cycle
                    #self.in_train = True
                    #self.model.train()
                    self.dl = self.data.train_dl
                    self.all_batches()

                    #valid cycle
                    if not self('begin_validate'):  #Will return False unless you want to skip validation
                        #self.in_train = False
                        #self.model.eval()
                        self.dl = self.data.valid_dl
                        with torch.no_grad(): self.all_batches() # May have to move torch no grad up
                self('after_epoch')

        except CancelTrainException:                            self('after_cancel_train')
        finally:                                                self('after_fit')

    def all_batches(self):
        try:
            for i, (xb, yb) in enumerate(self.dl):
                self.one_batch(i, xb, yb)
        except CancelEpochException: self('after_cancel_epoch')

    def one_batch(self, i, xb, yb):
        try:
            self.iter = i
            self.xb, self.yb = xb, yb;                          self('begin_batch')
            self.preds = self.model(self.xb);                   self('after_pred')
            self.loss = self.loss_func(self.preds, self.yb);    self('after_loss')
            if self.in_train:
                self.loss.backward();                           self('after_loss')
                self.opt.step();                                self('after_step')
                self.opt.zero_grad()

        except CancelBatchException:                            self('after_cancel_batch')

        finally:                                                self('after_batch')

    ALL_CALLBACKS = {'begin_batch', 'after_pred', 'after_loss', 'after_backward', 'after_step',
        'after_cancel_batch', 'after_batch', 'after_cancel_epoch', 'begin_fit',
        'begin_epoch', 'begin_validate', 'after_epoch',
        'after_cancel_train', 'after_fit'}

    def __call__(self, callback_name):
        result = False
        assert callback_name in self.ALL_CALLBACKS
        for cb in sorted(self.callbacks, key = lambda x: x._order): result = cb(callback_name) and result
        return result




class TrainEvalCallback(Callback):
    def begin_fit(self):
        self.learn.n_epochs = 0
        self.learn.n_iter = 0

    def begin_epoch(self):
        self.learn.n_epochs = self.epoch
        self.model.train()
        self.learn.in_train = True

    def begin_validate(self):
        self.model.eval()
        self.learn.in_train = False




class CancelTrainException(Exception): pass
class CancelEpochException(Exception): pass
class CancelBatchException(Exception): pass




class MoveBatchGPUCallback(Callback):
    _order = 100 #We want this to be done last, potentially (some operations may be requied before or after it)
    def begin_batch(self):
        self.learn.yb = self.yb.cuda()
        self.learn.xb = tuple(map(lambda x: x.cuda(), xb))
    def begin_fit(self):
        self.model.cuda()

class EarlyStoppingCallback(Callback):
    def __init__(self, max_epochs, max_batches):
        self.max_epochs = max_epochs
        self.max_batches = max_batches
    def after_epoch(self):
        if self.epoch >= self.max_epochs: raise CancelTrainException()
    def after_batch(self):
        if self.iter >= self.max_batches: raise CancelEpochException()

class OutputStepsCallback(Callback):
    def begin_batch(self): print(f"   Batch {self.iter}")
    def begin_epoch(self): print(F"Epoch {self.epoch}", "\nBeginning the Training")
    def begin_validate(self): print("Beginning the validation")

class SqueezePredsCallback(Callback):
    def after_pred(self):
        self.learn.preds = torch.squeeze(self.learn.preds, -1)

class AvgStats():
    def __init__(self, metrics, in_train):
        self.metrics, self.in_train = metrics, in_train
        self.reset_stats()

    def reset_stats(self):
        self.tot_loss, self.count = 0., 0
        self.tot_metrics = [0.]*len(self.metrics)

    @property
    def all_stats(self): return [self.tot_loss.item()] + self.tot_metrics

    @property
    def avg_stats(self): return [o/self.count for o in self.all_stats]

    def accumulate(self, learn):
        batch_size = learn.preds.shape[0]
        self.tot_loss += learn.loss * batch_size
        self.count += batch_size
        for i, m in enumerate(self.metrics):
            self.tot_metrics[i] += m(learn.preds, learn.yb)*batch_size


    def __repr__(self):
        if not self.count: return ""
        return f"{'train' if self.in_train else 'valid'}: {self.avg_stats}"


class AvgStatsCallback(Callback):
    def __init__(self, train_metrics, valid_metrics):
        self.train_stats, self.valid_stats = AvgStats(train_metrics, True), AvgStats(valid_metrics, False)

    def begin_fit(self):
        self.train_stats.reset_stats()
        self.valid_stats.reset_stats()

    def after_batch(self):
        current_stats = self.train_stats if self.in_train else self.valid_stats
        with torch.no_grad(): current_stats.accumulate(self.learn)

    def after_epoch(self):
        print(self.train_stats)
        print(self.valid_stats)


def annealer(f):
    def _inner(start, stop):
        return partial(f, start, stop)
    return _inner

@annealer
def sched_lin(start, stop, pos): return start + (stop-start)*pos
@annealer
def sched_cos(start, stop, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (stop-start) / 2
@annealer
def sched_none(start, stop, pos): return start
@annealer
def sched_exp(start, stop, pos): return (start)*(stop/start)**pos

def sched_combiner(pcts, scheds):
    assert sum(pcts) == 1
    pcts = torch.tensor([0.] + pcts)
    pcts_cum = torch.cumsum(pcts, 0)
    def _inner(pos):
        if pos>=1: pos = 0.99999 #Error on the unlikely chance it hits 1.0 exactly
        idx = (pos>=pcts_cum).nonzero().max()
        new_pos = (pos - pcts_cum[idx])/(pcts_cum[idx+1] - pcts_cum[idx])
        return scheds[idx](new_pos)
    return _inner


class ParamSchedulerCallback(Callback):
    _order = 1
    def __init__(self, pname, scheduler):
        self.pname, self.scheduler = pname, scheduler

    def begin_fit(self):
        setattr(self.opt, self.pname, self.scheduler(0.))

    def begin_batch(self):
        pos = self.epoch/self.num_epochs
        if hasattr(self.learn, 'n_batches'): #if you've included a way to measure the dataset length
            pos += self.iter/self.learn.n_batches/self.num_epochs
        setattr(self.opt, self.pname, self.scheduler(pos))

class BatchNumberCalculator(Callback):
    def begin_fit(self):
        ds_len = self.learn.data.train_dl.sampler.n
        bs = self.learn.data.train_dl.sampler.bs
        self.learn.n_batches = ds_len//bs



class RecorderCallback(Callback):
    def begin_fit(self):
        self.losses = [[],[]]
        self.lrs = [[],[]]

    def after_loss(self):
        train_valid_selector = 0 if self.in_train else 1
        self.losses[train_valid_selector].append(self.loss)
        self.lrs[train_valid_selector].append(self.opt.lr)
    def plot_lrs(self, train = True):
        train_valid_selector = 0 if train else 1
        plt.plot(self.lrs[train_valid_selector])

    def plot_losses(self, train = True):
        plt.plot(self.losses[train_valid_selector])