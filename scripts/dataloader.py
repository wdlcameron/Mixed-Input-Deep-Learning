
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/Part 1 - Dataloader.ipynb

import numpy as np
import torch
import pandas as pd
from pathlib import Path
import imageio
from skimage import io, transform
from torchvision import transforms

class Dataset():
    DROP_WARNING = 0.9

    def __init__(self, df_path, img_col, cont_cols, cat_cols, target_col, image_path, suffix = '.jpg', transforms = None):
        self.df_path = df_path
        self.img_col, self.cont_cols, self.cat_cols = img_col, cont_cols, cat_cols
        self.target_col = target_col
        self.suffix = suffix
        self.image_path = Path(image_path)
        self.transforms = transforms

        #read in the dataframe
        self.df = pd.read_csv(df_path)
        self.df = self.clean_dataframe(self.df)

    def __len__(self):
        return len(self.df)

    def clean_dataframe(self, df):
        orig_len = len(df)

        #Remove filenames for files that do not exist (or have errors in the filepath)
        existing_files = ((pd.Series([self.image_path]*len(df))/(df[self.img_col]+'.jpg'))
                          .apply(lambda x: self.check_path_valid(x)))

        df.drop(df[~existing_files].index, axis = 0, inplace = True)

        #Remove missing values from your target columns
        df.drop(df[df[self.target_col].isna()].index, axis = 0, inplace = True)

        df.reset_index(drop=True, inplace = True)
        if len(df)/orig_len < self.DROP_WARNING:
            print (f"Warning, more than {(1-self.DROP_WARNING)*100}% of your data was invalid")
        return df

    def check_path_valid(self, path):
        try: return path.exists()
        except: return False

    def __getitem__(self, idx):
        filename = self.df.loc[idx, self.img_col]
        cat_data = self.df.loc[idx, self.cat_cols]
        cont_data = self.df.loc[idx, self.cont_cols].values.astype(np.float32)
        target = self.df.loc[idx, self.target_col]

        tabular_data = torch.tensor(cont_data)
        target = torch.tensor(target)

        image = io.imread(self.image_path/(filename + self.suffix))
        if self.transforms: image = self.transforms(image)

        return (image, tabular_data), target

class Transforms():
    def __init__(self, transforms):
        self.transforms = transforms

    def __call__(self, x):
        for tsfm in self.transforms:
            x = tsfm(x)
        return x


class Resize():
    def __init__(self, size):
        self.size = size

    def __call__(self, img):
        return transform.resize(img, (self.size, self.size))


class ToTorch():
    def __init__(self):
        pass

    def __call__(self, img):
        return torch.tensor(img.transpose(2,0,1))

class Sampler():
    def __init__(self, dataset, bs, shuffle = True):
        self.dataset = dataset
        self.bs = bs
        self.shuffle = shuffle

        self.n = len(self.dataset)

    def __iter__(self):
        #idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)
        idxs = np.random.permutation(self.n) if self.shuffle else np.arange(self.n)
        for i in range(0, self.n, self.bs):
            yield idxs[i:i+self.bs]




def collate(data, transforms = None):
    xs, y = zip(*data)
    x1, x2 = zip(*xs)

    if transforms: x1 = transforms(x1)

    return (torch.stack(x1), torch.stack(x2)), torch.stack(y)

class DataLoader():
    def __init__(self, dataset, sampler, collate_func):
        self.dataset = dataset
        self.sampler = sampler
        self.collate_func = collate_func

    def __iter__(self):
        for idxs in self.sampler:
            minibatch = [self.dataset[idx] for idx in idxs]
            yield (self.collate_func(minibatch))

